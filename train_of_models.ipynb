{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this file we are going to train our four models with different combinations of preprocessing methods and vectorization methods for each model, so we can find out which combination of preprocessing and vectorization methods suits better for every type of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports \n",
    "import pickle\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from fasttext import FastText\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import path_to_data_folder, path_to_fast_text_model, glove_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD THE MODELS\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\") # model trained on lower case words, use lower case tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model = FastText.load_model(path_to_fast_text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(glove_path, 'rb') as file:\n",
    "#     glove_300d = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb Ячейка 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# make neccesary imports for preprocessing and vectorizatio\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctions_preprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m TextPreprocessor_flow_1, TextPreprocessor_flow_2, TextPreprocessor_flow_3, TextPreprocessor_flow_4, TextPreprocessor_flow_5, TextPreprocessor_flow_6, TextPreprocessor_flow_7, TextPreprocessor_flow_8, TextPreprocessor_flow_9, TextPreprocessor_flow_10, TextPreprocessor_flow_11, TextPreprocessor_flow_12, TextPreprocessor_flow_13, TextPreprocessor_flow_14, TextPreprocessor_flow_15, TextPreprocessor_flow_16, TextPreprocessor_flow_17, TextPreprocessor_flow_18, TextPreprocessor_flow_19, TextPreprocessor_flow_20, TextPreprocessor_flow_21\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctions_vectorization\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer, CountVectorizer, Word2VecVectorizer, FastTextVectorizer, GloveVectorizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m list_of_preprocessed_data \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mgender_df_preprocessed_0\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgender_df_preprocessed_1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgender_df_preprocessed_2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgender_df_preprocessed_3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgender_df_preprocessed_4\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgender_df_preprocessed_5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgender_df_preprocessed_6\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgender_df_preprocessed_7\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m list_of_vectorizers \u001b[39m=\u001b[39m [TfidfVectorizer, CountVectorizer, Word2VecVectorizer]\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/functions_vectorization.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m path_to_fast_text_model, glove_path\n\u001b[1;32m     12\u001b[0m \u001b[39m# Loads pre-trained word embedings model:\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m word2vec_model \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mword2vec-google-news-300\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# model trained on lower case words, use lower case tokens\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Load the pre-trained FastText model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m fast_model \u001b[39m=\u001b[39m FastText\u001b[39m.\u001b[39mload_model(path_to_fast_text_model)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/gensim/downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, BASE_DIR)\n\u001b[1;32m    502\u001b[0m module \u001b[39m=\u001b[39m \u001b[39m__import__\u001b[39m(name)\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39;49mload_data()\n",
      "File \u001b[0;32m~/gensim-data/word2vec-google-news-300/__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m():\n\u001b[1;32m      7\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_dir, \u001b[39m'\u001b[39m\u001b[39mword2vec-google-news-300\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mword2vec-google-news-300.gz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(path, binary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py:2065\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2062\u001b[0m kv \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(vector_size, vocab_size, dtype\u001b[39m=\u001b[39mdatatype)\n\u001b[1;32m   2064\u001b[0m \u001b[39mif\u001b[39;00m binary:\n\u001b[0;32m-> 2065\u001b[0m     _word2vec_read_binary(\n\u001b[1;32m   2066\u001b[0m         fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[1;32m   2067\u001b[0m     )\n\u001b[1;32m   2068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2069\u001b[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1960\u001b[0m, in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[1;32m   1958\u001b[0m new_chunk \u001b[39m=\u001b[39m fin\u001b[39m.\u001b[39mread(binary_chunk_size)\n\u001b[1;32m   1959\u001b[0m chunk \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_chunk\n\u001b[0;32m-> 1960\u001b[0m processed_words, chunk \u001b[39m=\u001b[39m _add_bytes_to_kv(\n\u001b[1;32m   1961\u001b[0m     kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[1;32m   1962\u001b[0m tot_processed_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m processed_words\n\u001b[1;32m   1963\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(new_chunk) \u001b[39m<\u001b[39m binary_chunk_size:\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1943\u001b[0m, in \u001b[0;36m_add_bytes_to_kv\u001b[0;34m(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1941\u001b[0m word \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39mlstrip(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1942\u001b[0m vector \u001b[39m=\u001b[39m frombuffer(chunk, offset\u001b[39m=\u001b[39mi_vector, count\u001b[39m=\u001b[39mvector_size, dtype\u001b[39m=\u001b[39mREAL)\u001b[39m.\u001b[39mastype(datatype)\n\u001b[0;32m-> 1943\u001b[0m _add_word_to_kv(kv, counts, word, vector, vocab_size)\n\u001b[1;32m   1944\u001b[0m start \u001b[39m=\u001b[39m i_vector \u001b[39m+\u001b[39m bytes_per_vector\n\u001b[1;32m   1945\u001b[0m processed_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1911\u001b[0m, in \u001b[0;36m_add_word_to_kv\u001b[0;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[1;32m   1909\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mduplicate word \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m in word2vec file, ignoring all but first\u001b[39m\u001b[39m\"\u001b[39m, word)\n\u001b[1;32m   1910\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 1911\u001b[0m word_id \u001b[39m=\u001b[39m kv\u001b[39m.\u001b[39;49madd_vector(word, weights)\n\u001b[1;32m   1913\u001b[0m \u001b[39mif\u001b[39;00m counts \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1914\u001b[0m     \u001b[39m# Most common scenario: no vocab file given. Just make up some bogus counts, in descending order.\u001b[39;00m\n\u001b[1;32m   1915\u001b[0m     \u001b[39m# TODO (someday): make this faking optional, include more realistic (Zipf-based) fake numbers.\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m     word_count \u001b[39m=\u001b[39m vocab_size \u001b[39m-\u001b[39m word_id\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py:562\u001b[0m, in \u001b[0;36mKeyedVectors.add_vector\u001b[0;34m(self, key, vector)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_to_key[target_index] \u001b[39m=\u001b[39m key\n\u001b[1;32m    561\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_to_index[key] \u001b[39m=\u001b[39m target_index\n\u001b[0;32m--> 562\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectors[target_index] \u001b[39m=\u001b[39m vector\n\u001b[1;32m    563\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    564\u001b[0m \u001b[39mreturn\u001b[39;00m target_index\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# make neccesary imports for preprocessing and vectorizatio\n",
    "from functions_preprocessing import TextPreprocessor_flow_1, TextPreprocessor_flow_2, TextPreprocessor_flow_3, TextPreprocessor_flow_4, TextPreprocessor_flow_5, TextPreprocessor_flow_6, TextPreprocessor_flow_7, TextPreprocessor_flow_8, TextPreprocessor_flow_9, TextPreprocessor_flow_10, TextPreprocessor_flow_11, TextPreprocessor_flow_12, TextPreprocessor_flow_13, TextPreprocessor_flow_14, TextPreprocessor_flow_15, TextPreprocessor_flow_16, TextPreprocessor_flow_17, TextPreprocessor_flow_18, TextPreprocessor_flow_19, TextPreprocessor_flow_20, TextPreprocessor_flow_21\n",
    "from functions_vectorization import TfidfVectorizer, CountVectorizer, Word2VecVectorizer, FastTextVectorizer, GloveVectorizer\n",
    "\n",
    "list_of_preprocessed_data = ['gender_df_preprocessed_0', 'gender_df_preprocessed_1', \"gender_df_preprocessed_2\", 'gender_df_preprocessed_3', 'gender_df_preprocessed_4', 'gender_df_preprocessed_5', 'gender_df_preprocessed_6', 'gender_df_preprocessed_7']\n",
    "list_of_vectorizers = [TfidfVectorizer, CountVectorizer, Word2VecVectorizer]\n",
    "list_of_models = [LogisticRegression, RandomForestClassifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, clean, split the data on which we are going to train the pipeline and evaluate\n",
    "#### Create different pipelines for future model training, testing, evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINES COBINATION AND IT'S SCORES FOR GENDER DATA\n",
    "created_pipelines_scores = {}\n",
    "for model in list_of_models:\n",
    "    for vectorizer in list_of_vectorizers:\n",
    "        for preprocessed_data in list_of_preprocessed_data:\n",
    "\n",
    "            pipeline_name = f\"pipeline_{preprocessed_data}_{vectorizer.__name__}_{model.__name__}\"\n",
    "\n",
    "            if vectorizer.__name__ == 'Word2VecVectorizer':\n",
    "                pipeline = Pipeline([\n",
    "                                ('vectorizer', vectorizer(word2vec_model)),\n",
    "                                ('model', model())\n",
    "                                ])\n",
    "            elif vectorizer.__name__ == 'GloveVectorizer':\n",
    "                pipeline = Pipeline([\n",
    "                                ('vectorizer', vectorizer()),\n",
    "                                ('model', model())\n",
    "                                ])\n",
    "            else:\n",
    "                pipeline = Pipeline([\n",
    "                                    ('vectorizer', vectorizer()),\n",
    "                                    ('model', model())\n",
    "                                    ])\n",
    "\n",
    "            df = pd.read_json(f'{preprocessed_data}')\n",
    "\n",
    "            X = df[f'{df.columns[0]}'].tolist()\n",
    "            y = df[f'{df.columns[1]}'].tolist()\n",
    "\n",
    "            # Split the dataset into training and testing sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            created_pipelines_scores[pipeline_name] = pipeline.score(X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'created_pipelines_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb Ячейка 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(created_pipelines_scores)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'created_pipelines_scores' is not defined"
     ]
    }
   ],
   "source": [
    "print(created_pipelines_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

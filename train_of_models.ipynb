{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this file we are going to train our four models with different combinations of preprocessing methods and vectorization methods for each model, so we can find out which combination of preprocessing and vectorization methods suits better for every type of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports \n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "\n",
    "from fasttext import FastText\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import path_to_data_folder, path_to_fast_text_model, glove_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD THE MODELS\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\") # model trained on lower case words, use lower case tokens\n",
    "# fast_model = FastText.load_model(path_to_fast_text_model)\n",
    "\n",
    "# with open(glove_path, 'rb') as file:\n",
    "#     glove_300d = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# make neccesary imports for preprocessing and vectorizatio\n",
    "from functions_preprocessing import TextPreprocessor_flow_1, TextPreprocessor_flow_2, TextPreprocessor_flow_3, TextPreprocessor_flow_4, TextPreprocessor_flow_5, TextPreprocessor_flow_6, TextPreprocessor_flow_7, TextPreprocessor_flow_8, TextPreprocessor_flow_9, TextPreprocessor_flow_10, TextPreprocessor_flow_11, TextPreprocessor_flow_12, TextPreprocessor_flow_13, TextPreprocessor_flow_14, TextPreprocessor_flow_15, TextPreprocessor_flow_16, TextPreprocessor_flow_17, TextPreprocessor_flow_18, TextPreprocessor_flow_19, TextPreprocessor_flow_20\n",
    "from functions_vectorization import TfidfVectorizer, CountVectorizer, Word2VecVectorizer, FastTextVectorizer, GloveVectorizer\n",
    "\n",
    "list_of_preprocessors = [TextPreprocessor_flow_1, TextPreprocessor_flow_2, TextPreprocessor_flow_3, TextPreprocessor_flow_4, TextPreprocessor_flow_5, TextPreprocessor_flow_6, TextPreprocessor_flow_7, TextPreprocessor_flow_8, TextPreprocessor_flow_9, TextPreprocessor_flow_10, TextPreprocessor_flow_11, TextPreprocessor_flow_12, TextPreprocessor_flow_13, TextPreprocessor_flow_14, TextPreprocessor_flow_15, TextPreprocessor_flow_16, TextPreprocessor_flow_17, TextPreprocessor_flow_18, TextPreprocessor_flow_19, TextPreprocessor_flow_20]\n",
    "list_of_vectorizers = [TfidfVectorizer, CountVectorizer, Word2VecVectorizer(word2vec_model)]\n",
    "list_of_models = [LogisticRegression, RandomForestClassifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create different pipelines for future model training, testing, evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINES FOR LOGISTICREGRESSION\n",
    "created_pipelines = {}\n",
    "for model in list_of_models:\n",
    "    for vectorizer in list_of_vectorizers:\n",
    "        for preprocessing_flow in list_of_preprocessors:\n",
    "\n",
    "            pipeline_name = f\"pipeline_{preprocessing_flow.__name__}_{vectorizer.__name__}_{model.__name__}\"\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                                ('preprocess', preprocessing_flow),\n",
    "                                ('vectorizer', vectorizer),\n",
    "                                ('model', model)\n",
    "                                ])\n",
    "\n",
    "            created_pipelines[pipeline_name] = pipeline\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pipeline_TextPreprocessor_flow_1_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_1'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_2_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_2'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_3_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_3'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_4_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_4'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_5_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_5'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_6_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_6'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_7_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_7'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_8_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_8'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_9_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_9'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_10_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_10'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_11_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_11'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_12_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_12'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_13_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_13'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_14_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_14'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_15_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_15'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_16_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_16'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_17_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_17'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_18_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_18'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_19_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_19'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)]), 'pipeline_TextPreprocessor_flow_20_TfidfVectorizer_LogisticRegression': Pipeline(steps=[('preprocess',\n",
      "                 <class 'functions_preprocessing.TextPreprocessor_flow_20'>),\n",
      "                ('vectorizer',\n",
      "                 <class 'sklearn.feature_extraction.text.TfidfVectorizer'>),\n",
      "                ('model',\n",
      "                 <class 'sklearn.linear_model._logistic.LogisticRegression'>)])}\n"
     ]
    }
   ],
   "source": [
    "print(created_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPARE THE DATA FOR FITTING THE FUTURE MODELS\n",
    "# Reading datasets\n",
    "gender_df = pd.read_csv(f'{path_to_data_folder}/gender.csv')\n",
    "\n",
    "jud_per_df = pd.read_csv(f'{path_to_data_folder}/judging_perceiving.csv')\n",
    "\n",
    "political_df  = pd.read_csv(f'{path_to_data_folder}/political_leaning.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = gender_df['post'].tolist()  # Replace 'text_column' with the column containing text data\n",
    "y = gender_df['female'].tolist()  # Replace 'target_column' with the column containing target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35708, 35708, 8927, 8927)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb Ячейка 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m created_pipelines[\u001b[39m'\u001b[39;49m\u001b[39mpipeline_TextPreprocessor_flow_10_TfidfVectorizer_LogisticRegression\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:423\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    422\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 423\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    424\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:377\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    375\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    376\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    378\u001b[0m     cloned_transformer,\n\u001b[1;32m    379\u001b[0m     X,\n\u001b[1;32m    380\u001b[0m     y,\n\u001b[1;32m    381\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    382\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    383\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    384\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    385\u001b[0m )\n\u001b[1;32m    386\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 957\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    958\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "created_pipelines['pipeline_TextPreprocessor_flow_10_TfidfVectorizer_LogisticRegression'].fit(X_train, y_train)\n",
    "\n",
    "scores = {}\n",
    "\n",
    "# Future script\n",
    "for pipeline_name in created_pipelines:\n",
    "    created_pipelines[pipeline_name].fit(X_train, y_train)\n",
    "    scores[pipeline_name] = created_pipelines[pipeline_name].score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

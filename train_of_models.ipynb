{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this file we are going to train our four models with different combinations of preprocessing methods and vectorization methods for each model, so we can find out which combination of preprocessing and vectorization methods suits better for every type of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports \n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "\n",
    "from fasttext import FastText\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import path_to_data_folder, path_to_fast_text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD THE MODELS\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\") # model trained on lower case words, use lower case tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model = FastText.load_model(path_to_fast_text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(glove_path, 'rb') as file:\n",
    "#     glove_300d = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make neccesary imports for preprocessing and vectorizatio\n",
    "from functions_vectorization import TfidfVectorizer, CountVectorizer, Word2VecVectorizer, FastTextVectorizer\n",
    "\n",
    "list_of_preprocessed_data = ['/gender_df_preprocessed_0', '/gender_df_preprocessed_1', \"/gender_df_preprocessed_2\", '/gender_df_preprocessed_3', '/gender_df_preprocessed_4', '/gender_df_preprocessed_5', '/gender_df_preprocessed_6', '/gender_df_preprocessed_7', '/gender_df_preprocessed_9', '/gender_df_preprocessed_10', '/gender_df_preprocessed_11', '/gender_df_preprocessed_12', '/gender_df_preprocessed_13', '/gender_df_preprocessed_14', '/gender_df_preprocessed_15', '/gender_df_preprocessed_16', '/gender_df_preprocessed_17', '/gender_df_preprocessed_18']\n",
    "list_of_vectorizers = [CountVectorizer]\n",
    "list_of_models = [RandomForestClassifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, clean, split the data on which we are going to train the pipeline and evaluate\n",
    "#### Create different pipelines for future model training, testing, evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb Ячейка 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m created_pipelines_scores[pipeline_name] \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m tqdm_desc \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpreprocessed_data\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mvectorizer\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:423\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    422\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 423\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    424\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:377\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    375\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    376\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    378\u001b[0m     cloned_transformer,\n\u001b[1;32m    379\u001b[0m     X,\n\u001b[1;32m    380\u001b[0m     y,\n\u001b[1;32m    381\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    382\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    383\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    384\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    385\u001b[0m )\n\u001b[1;32m    386\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 957\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    958\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1402\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1401\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[0;32m-> 1402\u001b[0m X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_limit_features(\n\u001b[1;32m   1403\u001b[0m     X, vocabulary, max_doc_count, min_doc_count, max_features\n\u001b[1;32m   1404\u001b[0m )\n\u001b[1;32m   1405\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1406\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1246\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1244\u001b[0m new_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcumsum(mask) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# maps old indices to new\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m removed_terms \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m-> 1246\u001b[0m \u001b[39mfor\u001b[39;00m term, old_index \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39;49m(vocabulary\u001b[39m.\u001b[39;49mitems()):\n\u001b[1;32m   1247\u001b[0m     \u001b[39mif\u001b[39;00m mask[old_index]:\n\u001b[1;32m   1248\u001b[0m         vocabulary[term] \u001b[39m=\u001b[39m new_indices[old_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PIPELINES COBINATION AND IT'S SCORES FOR GENDER DATA\n",
    "created_pipelines_scores = {}\n",
    "for model in list_of_models:\n",
    "    for vectorizer in list_of_vectorizers:\n",
    "        for preprocessed_data in list_of_preprocessed_data:\n",
    "\n",
    "            pipeline_name = f\"pipeline_{preprocessed_data}_{vectorizer.__name__}_{model.__name__}\"\n",
    "\n",
    "            if vectorizer.__name__ == 'Word2VecVectorizer':\n",
    "                pipeline = Pipeline([\n",
    "                                ('vectorizer', vectorizer(word2vec_model)),\n",
    "                                ('model', model())\n",
    "                                ])\n",
    "            elif vectorizer.__name__ == 'FastTextVectorizer':\n",
    "                pipeline = Pipeline([\n",
    "                                ('vectorizer', vectorizer(fast_model)),\n",
    "                                ('model', model())\n",
    "                                ])\n",
    "            else:\n",
    "                pipeline = Pipeline([\n",
    "                                    ('vectorizer', vectorizer()),\n",
    "                                    ('model', model())\n",
    "                                    ])\n",
    "\n",
    "            df = pd.read_json(f'{path_to_data_folder}{preprocessed_data}')\n",
    "\n",
    "            X = df[f'{df.columns[0]}'].tolist()\n",
    "            y = df[f'{df.columns[1]}'].tolist()\n",
    "\n",
    "            # Split the dataset into training and testing sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            created_pipelines_scores[pipeline_name] = pipeline.score(X_test, y_test)\n",
    "\n",
    "            tqdm_desc = f\"{preprocessed_data}_{vectorizer.__name__}_{model.__name__}\"\n",
    "            tqdm.write(f\"Finished: {tqdm_desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pipeline_/gender_df_preprocessed_0_CountVectorizer_RandomForestClassifier': 0.8208804749635936}\n"
     ]
    }
   ],
   "source": [
    "print(created_pipelines_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

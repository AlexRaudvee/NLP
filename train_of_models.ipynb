{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this file we are going to train our four models with different combinations of preprocessing methods and vectorization methods for each model, so we can find out which combination of preprocessing and vectorization methods suits better for every type of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import path_to_data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": " cannot be opened for loading!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb Ячейка 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# make neccesary imports for preprocessing and vectorization\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctions_preprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m flow_preprocessing_1, flow_preprocessing_2, flow_preprocessing_3, flow_preprocessing_4, flow_preprocessing_5, flow_preprocessing_6, flow_preprocessing_7, flow_preprocessing_8, flow_preprocessing_9, flow_preprocessing_10, flow_preprocessing_11, flow_preprocessing_12, flow_preprocessing_13, flow_preprocessing_14, flow_preprocessing_15, flow_preprocessing_16, flow_preprocessing_17, flow_preprocessing_18, flow_preprocessing_19, flow_preprocessing_20\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctions_vectorization\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer, CountVectorizer, Word2VecVectorizer, FastText\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m list_of_preprocessing_functions \u001b[39m=\u001b[39m [flow_preprocessing_1, flow_preprocessing_2, flow_preprocessing_3, flow_preprocessing_4, flow_preprocessing_5, flow_preprocessing_6, flow_preprocessing_7, flow_preprocessing_8, flow_preprocessing_9, flow_preprocessing_10, flow_preprocessing_11, flow_preprocessing_12, flow_preprocessing_13, flow_preprocessing_14, flow_preprocessing_15, flow_preprocessing_16, flow_preprocessing_17, flow_preprocessing_18, flow_preprocessing_19, flow_preprocessing_20]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m list_of_vectorizers \u001b[39m=\u001b[39m [TfidfVectorizer, CountVectorizer, Word2VecVectorizer, FastText]\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/functions_vectorization.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Load the pre-trained FastText model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model_path \u001b[39m=\u001b[39m path_to_fast_text_model\n\u001b[0;32m---> 17\u001b[0m fast_model \u001b[39m=\u001b[39m FastText\u001b[39m.\u001b[39;49mload_model(model_path)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Count based and frequency methods\u001b[39;00m\n\u001b[1;32m     20\u001b[0m tfidf_vect \u001b[39m=\u001b[39m TfidfVectorizer()\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/fasttext/FastText.py:441\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m eprint(\u001b[39m\"\u001b[39m\u001b[39mWarning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m \u001b[39mreturn\u001b[39;00m _FastText(model_path\u001b[39m=\u001b[39;49mpath)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/fasttext/FastText.py:98\u001b[0m, in \u001b[0;36m_FastText.__init__\u001b[0;34m(self, model_path, args)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fasttext\u001b[39m.\u001b[39mfasttext()\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m model_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf\u001b[39m.\u001b[39;49mloadModel(model_path)\n\u001b[1;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_words \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m:  cannot be opened for loading!"
     ]
    }
   ],
   "source": [
    "# make neccesary imports for preprocessing and vectorization\n",
    "from functions_preprocessing import flow_preprocessing_1, flow_preprocessing_2, flow_preprocessing_3, flow_preprocessing_4, flow_preprocessing_5, flow_preprocessing_6, flow_preprocessing_7, flow_preprocessing_8, flow_preprocessing_9, flow_preprocessing_10, flow_preprocessing_11, flow_preprocessing_12, flow_preprocessing_13, flow_preprocessing_14, flow_preprocessing_15, flow_preprocessing_16, flow_preprocessing_17, flow_preprocessing_18, flow_preprocessing_19, flow_preprocessing_20\n",
    "from functions_vectorization import TfidfVectorizer, CountVectorizer, Word2VecVectorizer, FastText\n",
    "\n",
    "list_of_preprocessing_functions = [flow_preprocessing_1, flow_preprocessing_2, flow_preprocessing_3, flow_preprocessing_4, flow_preprocessing_5, flow_preprocessing_6, flow_preprocessing_7, flow_preprocessing_8, flow_preprocessing_9, flow_preprocessing_10, flow_preprocessing_11, flow_preprocessing_12, flow_preprocessing_13, flow_preprocessing_14, flow_preprocessing_15, flow_preprocessing_16, flow_preprocessing_17, flow_preprocessing_18, flow_preprocessing_19, flow_preprocessing_20]\n",
    "list_of_vectorizers = [TfidfVectorizer, CountVectorizer, Word2VecVectorizer, FastText]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change the path to the data folder according to your device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPARE THE DATA FOR FITTING THE FUTURE MODELS\n",
    "# Reading datasets\n",
    "\n",
    "# path to the data folder (change for your device)\n",
    "path_data = '/Users/alexraudvee/Desktop/lai-data/'\n",
    "\n",
    "gender_df = pd.read_csv(f'{path_data}gender.csv')\n",
    "\n",
    "jud_per_df = pd.read_csv(f'{path_data}judging_perceiving.csv')\n",
    "\n",
    "political_df  = pd.read_csv(f'{path_data}political_leaning.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = gender_df['post']  # Replace 'text_column' with the column containing text data\n",
    "y = gender_df['female']  # Replace 'target_column' with the column containing target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create different pipelines for future model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINES FOR LOGISTICREGRESSION\n",
    "created_pipelines_LogisticRegression = {}\n",
    "\n",
    "for vectorizer in list_of_vectorizers:\n",
    "    for preprocessing_flow_function in list_of_preprocessing_functions:\n",
    "\n",
    "        pipeline_name = f\"pipeline_{preprocessing_flow_function.__name__}_{vectorizer.__name__}_Logreg\"\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "        ('preprocess', vectorizer(preprocessor=preprocessing_flow_function)),\n",
    "        ('model', LogisticRegression())])\n",
    "\n",
    "        created_pipelines_LogisticRegression[pipeline_name] = pipeline\n",
    "\n",
    "# PIPELINES FOR MODEL2\n",
    "created_pipelines_Model2 = {}\n",
    "\n",
    "for vectorizer in list_of_vectorizers:\n",
    "    for preprocessing_flow_function in list_of_preprocessing_functions:\n",
    "\n",
    "        pipeline_name = f\"pipeline_{preprocessing_flow_function.__name__}_{vectorizer.__name__}_Logreg\"\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "        ('preprocess', vectorizer(preprocessor=preprocessing_flow_function)),\n",
    "        ('model', LogisticRegression())])\n",
    "\n",
    "        created_pipelines_Model2[pipeline_name] = pipeline\n",
    "\n",
    "# PIPELINES FOR MODEL3\n",
    "created_pipelines_Model3 = {}\n",
    "\n",
    "for vectorizer in list_of_vectorizers:\n",
    "    for preprocessing_flow_function in list_of_preprocessing_functions:\n",
    "\n",
    "        pipeline_name = f\"pipeline_{preprocessing_flow_function.__name__}_{vectorizer.__name__}_Logreg\"\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "        ('preprocess', vectorizer(preprocessor=preprocessing_flow_function)),\n",
    "        ('model', LogisticRegression())])\n",
    "\n",
    "        created_pipelines_Model3[pipeline_name] = pipeline\n",
    "\n",
    "# PIPELINES FOR MODEL4\n",
    "created_pipelines_Model4 = {}\n",
    "\n",
    "for vectorizer in list_of_vectorizers:\n",
    "    for preprocessing_flow_function in list_of_preprocessing_functions:\n",
    "\n",
    "        pipeline_name = f\"pipeline_{preprocessing_flow_function.__name__}_{vectorizer.__name__}_Logreg\"\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "        ('preprocess', vectorizer(preprocessor=preprocessing_flow_function)),\n",
    "        ('model', LogisticRegression())])\n",
    "\n",
    "        created_pipelines_Model4[pipeline_name] = pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb Ячейка 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipeline_1 \u001b[39m=\u001b[39m created_pipelines_TfidfVect_Logistic_reg[\u001b[39m'\u001b[39m\u001b[39mpipeline_flow_preprocessing_1_Tfifd_Logreg\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/train_of_models.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pipeline_1\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:423\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    422\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 423\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    424\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:377\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    375\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    376\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    378\u001b[0m     cloned_transformer,\n\u001b[1;32m    379\u001b[0m     X,\n\u001b[1;32m    380\u001b[0m     y,\n\u001b[1;32m    381\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    382\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    383\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    384\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    385\u001b[0m )\n\u001b[1;32m    386\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 957\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    958\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2134\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2135\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2136\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2137\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2138\u001b[0m )\n\u001b[0;32m-> 2139\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2141\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2142\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    110\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "pipeline_1 = created_pipelines_LogisticRegression['pipeline_flow_preprocessing_1_TfifdVectorizer_Logreg']\n",
    "pipeline_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

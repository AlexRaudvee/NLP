{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dog be bark loudly outside .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download WordNet data (needed for lemmatization) and punkt for normal functioning of tokinizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in tokens]\n",
    "    \n",
    "    # Join lemmatized words back into a string\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "# Example text\n",
    "text = \"The dogs are barking loudly outside.\"\n",
    "\n",
    "# Lemmatize the text\n",
    "lemmatized_text = lemmatize_text(text)\n",
    "print(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text with remove_stop_words: ['example', 'text', '.']\n",
      "Processed text with remove_upercase: ['this', 'is', 'an', 'example', 'text', '.']\n",
      "Processed text with remove_punctuation: ['This', 'is', 'an', 'example', 'text']\n",
      "Processed text with remove_stop_words -> remove_upercase: ['example', 'text', '.']\n",
      "Processed text with remove_stop_words -> remove_punctuation: ['example', 'text']\n",
      "Processed text with remove_upercase -> remove_punctuation: ['this', 'is', 'an', 'example', 'text']\n",
      "Processed text with remove_stop_words -> remove_upercase -> remove_punctuation: ['example', 'text']\n"
     ]
    }
   ],
   "source": [
    "from functions import tokenizer, remove_stop_words, remove_upercase, remove_punctuation\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Define your functions\n",
    "def function_1(text):\n",
    "    return text.upper()\n",
    "\n",
    "def function_2(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Define more functions (function_3 to function_16) in a similar manner\n",
    "\n",
    "# Store your functions in a list\n",
    "function_list = [remove_stop_words, remove_upercase, remove_punctuation]  # Add all your functions here\n",
    "\n",
    "# Example text\n",
    "input_text = \"This is an example text.\"\n",
    "\n",
    "# Apply functions in parallel\n",
    "# Generate combinations of functions and apply them to the text\n",
    "for r in range(1, len(function_list) + 1):\n",
    "    for combination in itertools.combinations(function_list, r):\n",
    "        processed_text = input_text\n",
    "        for func in combination:\n",
    "            processed_text = func(processed_text)\n",
    "            \n",
    "        if type(processed_text) is str:\n",
    "            processed_text = tokenizer(processed_text)\n",
    "\n",
    "        print(f\"Processed text with {' -> '.join(f.__name__ for f in combination)}:\", processed_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\") # model trained on lower case words, use lower case tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from functions_preprocessing import flow_preprocessing_1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sample data\n",
    "X = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "y = [1, 1, 0]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Word2VecVectorizer\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # gets the token that are in the model\n",
    "        document_embeddings = [np.mean([self.word2vec[token] for token in document if token in self.word2vec], axis=0) \n",
    "                               for document in X]\n",
    "\n",
    "        return np.array(document_embeddings)\n",
    "\n",
    "# Text Preprocessing Transformer\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        processed_text = [self.preprocess_text(text) for text in X]\n",
    "        return processed_text\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        tokens = flow_preprocessing_1(text=text)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# Create the pipeline\n",
    "word2vec_pipeline = Pipeline([\n",
    "    ('preprocess', TextPreprocessor()),\n",
    "    ('vectorizer', Word2VecVectorizer(word2vec_model)),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipelines = {\"pipe\": word2vec_pipeline, 'pipe2': word2vec_pipeline}\n",
    "\n",
    "# Transform the training data\n",
    "pipelines['pipe2'].fit(X_train, y_train)\n",
    "word2vec_accuracy = word2vec_pipeline.score(X_test, y_test)\n",
    "print(word2vec_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexraudvee/Desktop/NLP_fasttext_development/debug_and_test.ipynb Ячейка 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/debug_and_test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/debug_and_test.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctions_vectorization\u001b[39;00m \u001b[39mimport\u001b[39;00m GloveVectorizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexraudvee/Desktop/NLP_fasttext_development/debug_and_test.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m GloveVectorizer\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/functions_vectorization.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m word2vec_model \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mword2vec-google-news-300\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# model trained on lower case words, use lower case tokens\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Load the pre-trained FastText model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m fast_model \u001b[39m=\u001b[39m FastText\u001b[39m.\u001b[39;49mload_model(path_to_fast_text_model)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Count based and frequency methods\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tfidf_vect \u001b[39m=\u001b[39m TfidfVectorizer()\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/fasttext/FastText.py:436\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(path):\n\u001b[1;32m    435\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mreturn\u001b[39;00m _FastText(model_path\u001b[39m=\u001b[39;49mpath)\n",
      "File \u001b[0;32m~/Desktop/NLP_fasttext_development/.venv/lib/python3.9/site-packages/fasttext/FastText.py:94\u001b[0m, in \u001b[0;36m_FastText.__init__\u001b[0;34m(self, model_path, args)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fasttext\u001b[39m.\u001b[39mfasttext()\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m model_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf\u001b[39m.\u001b[39;49mloadModel(model_path)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_words \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

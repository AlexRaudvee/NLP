{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexraudvee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dog be bark loudly outside .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download WordNet data (needed for lemmatization) and punkt for normal functioning of tokinizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in tokens]\n",
    "    \n",
    "    # Join lemmatized words back into a string\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "# Example text\n",
    "text = \"The dogs are barking loudly outside.\"\n",
    "\n",
    "# Lemmatize the text\n",
    "lemmatized_text = lemmatize_text(text)\n",
    "print(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text with remove_stop_words: ['example', 'text', '.']\n",
      "Processed text with remove_upercase: ['this', 'is', 'an', 'example', 'text', '.']\n",
      "Processed text with remove_punctuation: ['This', 'is', 'an', 'example', 'text']\n",
      "Processed text with remove_stop_words -> remove_upercase: ['example', 'text', '.']\n",
      "Processed text with remove_stop_words -> remove_punctuation: ['example', 'text']\n",
      "Processed text with remove_upercase -> remove_punctuation: ['this', 'is', 'an', 'example', 'text']\n",
      "Processed text with remove_stop_words -> remove_upercase -> remove_punctuation: ['example', 'text']\n"
     ]
    }
   ],
   "source": [
    "from functions import tokenizer, remove_stop_words, remove_upercase, remove_punctuation\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Define your functions\n",
    "def function_1(text):\n",
    "    return text.upper()\n",
    "\n",
    "def function_2(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Define more functions (function_3 to function_16) in a similar manner\n",
    "\n",
    "# Store your functions in a list\n",
    "function_list = [remove_stop_words, remove_upercase, remove_punctuation]  # Add all your functions here\n",
    "\n",
    "# Example text\n",
    "input_text = \"This is an example text.\"\n",
    "\n",
    "# Apply functions in parallel\n",
    "# Generate combinations of functions and apply them to the text\n",
    "for r in range(1, len(function_list) + 1):\n",
    "    for combination in itertools.combinations(function_list, r):\n",
    "        processed_text = input_text\n",
    "        for func in combination:\n",
    "            processed_text = func(processed_text)\n",
    "            \n",
    "        if type(processed_text) is str:\n",
    "            processed_text = tokenizer(processed_text)\n",
    "\n",
    "        print(f\"Processed text with {' -> '.join(f.__name__ for f in combination)}:\", processed_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\") # model trained on lower case words, use lower case tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from functions_preprocessing import flow_preprocessing_1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from functions_preprocessing import remove_emoji, remove_usernames, remove_hashtags, remove_url, tokenizer\n",
    "\n",
    "# Sample data\n",
    "X = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "y = [1, 1, 0]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Word2VecVectorizer\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # gets the token that are in the model\n",
    "        document_embeddings = [np.mean([self.word2vec[token] for token in document if token in self.word2vec], axis=0) \n",
    "                               for document in X]\n",
    "\n",
    "        return np.array(document_embeddings)\n",
    "\n",
    "# Text Preprocessing Transformer\n",
    "class TextPreprocessor_flow_1(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        processed_text = [self.preprocess_text(text) for text in X]\n",
    "        return processed_text\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        return remove_emoji(remove_usernames(remove_hashtags(remove_url(text))))\n",
    "\n",
    "# Create the pipeline\n",
    "word2vec_pipeline = Pipeline([\n",
    "    ('preprocess', TextPreprocessor_flow_1()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipelines = {\"pipe\": word2vec_pipeline, 'pipe2': word2vec_pipeline}\n",
    "\n",
    "# Transform the training data\n",
    "word2vec_pipeline.fit(X_train, y_train)\n",
    "word2vec_accuracy = word2vec_pipeline.score(X_test, y_test)\n",
    "print(word2vec_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# New data to add in each iteration (example)\n",
    "new_people = ['pipe name', [\"scores\"]]\n",
    "\n",
    "# File path of the existing CSV file\n",
    "file_path = \"data.csv\"\n",
    "\n",
    "# Append new data to the CSV file\n",
    "with open(file_path, 'a', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(new_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>score_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pipe name</td>\n",
       "      <td>['scores']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pipe name</td>\n",
       "      <td>['scores']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pipe name</td>\n",
       "      <td>['scores']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pipeline_name  score_data\n",
       "0     pipe name  ['scores']\n",
       "1     pipe name  ['scores']\n",
       "2     pipe name  ['scores']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'the', 'first', 'document', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextPreprocessor_flow_1().preprocess_text(text = \"This is the first document#chill.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preprocessed_data_2 = pd.read_json('gender_df_preprocessed_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data_2.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_gender = preprocessed_data_2['post'].tolist()\n",
    "y_gender = preprocessed_data_2['female'].tolist()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_gender, X_test_gender, y_train_gender, y_test_gender = train_test_split(X_gender, y_gender, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35708, 35708, 8927, 8927)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_gender), len(y_train_gender), len(X_test_gender), len(y_test_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good on you for being responsible! I know self...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>must go to the grocery store with their child,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>things on her videos, and YouTube took the vid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>their app. There's also a program called SYNC ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>side. If the cops don't take your side, you'll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44630</th>\n",
       "      <td>if smegma kept her kids away just out of spite...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44631</th>\n",
       "      <td>PhDs to change the time on my microwave. I did...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44632</th>\n",
       "      <td>HiLIARy could even think of doing! I think Car...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44633</th>\n",
       "      <td>of the hand is a breeze. It swells after thoug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44634</th>\n",
       "      <td>actual real tears. Only pretend crying. Good b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44635 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    post  female\n",
       "0      Good on you for being responsible! I know self...       1\n",
       "1      must go to the grocery store with their child,...       1\n",
       "2      things on her videos, and YouTube took the vid...       1\n",
       "3      their app. There's also a program called SYNC ...       1\n",
       "4      side. If the cops don't take your side, you'll...       1\n",
       "...                                                  ...     ...\n",
       "44630  if smegma kept her kids away just out of spite...       1\n",
       "44631  PhDs to change the time on my microwave. I did...       1\n",
       "44632  HiLIARy could even think of doing! I think Car...       1\n",
       "44633  of the hand is a breeze. It swells after thoug...       1\n",
       "44634  actual real tears. Only pretend crying. Good b...       1\n",
       "\n",
       "[44635 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035510249803965"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "preprocessed_data_2 = pd.read_json('gender_df_preprocessed_0')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_gender = preprocessed_data_2['post'].tolist()\n",
    "y_gender = preprocessed_data_2['female'].tolist()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_gender, X_test_gender, y_train_gender, y_test_gender = train_test_split(X_gender, y_gender, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create the pipeline\n",
    "_pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# fit the pipeline\n",
    "_pipeline.fit(X_train_gender, y_train_gender)\n",
    "\n",
    "# test pipeline\n",
    "_pipeline.score(X_test_gender, y_test_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import demoji\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "\n",
    "def flow_preprocessing_12(text: str) -> list[str]:\n",
    "    parsed = urlparse(text)\n",
    "    # remove url\n",
    "    text = text.replace(parsed.scheme + \"://\" + parsed.netloc, \"\")\n",
    "    # remove hashtags\n",
    "    text = re.sub(r'#\\w+\\b', '', text)\n",
    "    # remove usernames\n",
    "    text = re.sub(r'@\\w+\\b', '', text)\n",
    "    # remove emoji\n",
    "    text = demoji.replace(text, '')\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    text = [porter.stem(word) for word in text.split()]\n",
    "\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Tokens: the big cat is running in the park Soviet Union\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def unite_noun_neighbors(words):\n",
    "    tagged_words = pos_tag(words)\n",
    "    unified_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tagged_words):\n",
    "        if tagged_words[i][1].startswith('NN'):\n",
    "            current_nouns = [tagged_words[i][0]]\n",
    "            j = i + 1\n",
    "            while j < len(tagged_words) and tagged_words[j][1].startswith('NN'):\n",
    "                current_nouns.append(tagged_words[j][0])\n",
    "                j += 1\n",
    "            unified_tokens.append(' '.join(current_nouns))\n",
    "            i = j\n",
    "        else:\n",
    "            unified_tokens.append(tagged_words[i][0])\n",
    "            i += 1\n",
    "    return \n",
    "\n",
    "# Example sentence as a list of tokenized words\n",
    "tokenized_sentence = [\"the\", \"big\", \"cat\", \"is\", \"running\", \"in\", \"the\", \"park\", \"Soviet\", \"Union\"]\n",
    "\n",
    "unified_tokens = unite_noun_neighbors(tokenized_sentence)\n",
    "print(\"Unified Tokens:\", unified_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
